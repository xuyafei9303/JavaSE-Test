Java中的锁：
    锁的主要作用是保障线程在并发情况下数据的一致性。多线程编程中为了保障数据的一致性，通常需要在使用对象或者调用方法之前加锁，这时候如果有其他线程也需要使用该对象或者调用该方法，则首先需要
    获得锁，如果某个线程发现锁正在被其他线程使用，就会进入阻塞队列等待锁的释放，直到其他线程执行完并释放锁，该线程才有机会再次获取锁并执行操作。

    锁从乐观和悲观的角度可分为乐观锁和悲观锁，从获取资源的公平性角度可分为公平锁和非公平锁，从是否共享资源的角度可分为共享锁和独占锁，从锁的状态角度可分为偏向锁、轻量级锁、重量级锁。
    jvm还巧妙的设计了自旋锁以更快的使用CPU资源

    乐观锁：
        每次读取数据时都会认为别人不会修改数据，所以不会加锁，但在更新的时候会判断在此期间别人有没有更新该数据，通常采用在写时先读出当前版本号然后加锁的方法。
        具体过程：比较当前版本号与上一次的版本号，如果版本号一致，代表没有被其他人修改，则进行更新，如果版本号不一致，则重复进行读，比较，写操作
        CAS：Java中的乐观锁大部分都是采用CAS（Compare And Swap）比较和交换操作来实现的，CAS是一种原子操作，在对数据操作之前首先会比较当前值和传入的值是否一致，如果一致就进行更新，如果不一致返回失败

    悲观锁：
        每次读取数据时都认为别人会修改数据，所以每次在读写数据时都会加锁，这样别人想读写这个数据就会阻塞，等待直到获取到锁
        Java中的悲观锁大部分都是使用AQS（Abstract Queued Synchronized，抽象的队列同步器）架构来实现的。该架构下的锁会尝试以CAS乐观锁去获取锁，如果获取不到，则会转为悲观锁（如RetreenLock）

    自旋锁：
        如果持有锁的线程能够在很短的时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态的切换进入阻塞、挂起状态，只需要自旋等一等，在等待持有锁的线程释放锁后即可立即获取锁，这样就避免了用户线程在用户态
        和内核态之间的频繁切换而导致的时间消耗
        线程在自旋的时候会占用CPU，在线程长时间自旋获取不到锁时，将会产生CPU浪费，甚至有时线程永远无法获取锁而导致CPU资源被永久占用，所以需要设定一个自旋等待时间，超出时间就退出自旋模式并释放其持有的锁

        1.自旋的优缺点：
            优点：自旋锁可以减少CPU上下文的切换，对于占用锁的时间非常短或锁竞争不激烈的代码块来说性能大幅度提升，因为自旋的CPU耗时明显少于线程阻塞、挂起、在唤醒时两次CPU上下文切换所用的时间
            缺点：在持有锁的线程占用锁时间过长或锁的竞争过于激烈时，线程在自旋过程中会长时间获取不到锁资源，将引起CPU的浪费，系统有复杂的锁依赖的情况下不适合使用自旋锁
        2.自旋锁的时间阈值：
            自旋锁用于使当前线程占着CPU的资源不释放，等到下次自旋获取锁资源后立即执行相关操作。但是怎么选择自旋的执行时间呢？如果自旋执行的时间太长，则会有大量的线程处于自旋状态且占用CPU资源，造成系统资源浪费。
            JDK1.6以后，引入了自适应自旋锁。自适应自旋锁的自旋时间不再是固定值，而是由上一次在同一个锁上的自旋时间及锁的拥有者的状态来决定的，可基本认为一个线程上下文切换的时间就是一个锁自旋的最佳时间。

    synchronized：
        用于为Java对象、方法、代码块提供线程安全的操作。属于独占式的悲观锁，同时属于可重入锁。
        使用synchronized关键字修饰对象时，同一时刻只能有一个线程对该对象进行访问；修饰方法、代码块时，同一时刻只能有一个线程执行该方法体或代码块，其他线程只能等待当前线程执行完毕并释放锁资源后才能访问该对象或代码块
        Java中的每个对象都有个monitor对象，加锁就是在竞争monitor对象。对代码块加锁就是通过在前后分别加上monitorEnter和monitorExit指令，对方是否加锁是通过一个标记位来判断的。
        作用范围：
            1.作用于成员变量和非静态方法时，锁住的是对象的实例，即this对象
            2.作用于静态方法，锁住的事class实例，因为静态方法属于class而不属于对象
            3.作用于一个代码块时，锁住的是所有代码块中配置的对象
        实现原理：
            synchronized内部包括ContentionList、EntryList、WaitSet、OnDeck、Owner、!Owner六大区域
            1.ContentionList：锁竞争队列，所有请求锁的线程都会被放在竞争队列中
            2.EntryList：竞争候选队列，在ContentionList中有资格成为候选者来竞争锁资源的线程被移动到了EntryList
            3.WaitSet：等待集合，调用wait方法后被阻塞的线程将被放在WaitSet中
            4.OnDeck：竞争候选者，在同一时刻最多只有一个线程在竞争锁资源，该线程的状态被称为OnDeck
            5.Owner：竞争到锁资源的线程被称为Owner状态线程
            6.!Owner：在Owner线程释放锁后，会从Owner的状态变成!Owner
        synchronized在收到新的锁请求时首先自旋，如果通过自旋也没有获取锁资源，则将被放入锁竞争队列ContentionList中。为了防止锁竞争时ContentionList尾部的元素被大量的并发线程进行CAS访问而影响性能，Owner线程会在释放锁资源时将ContentionList中
        的部分线程移动到EntryList中，并指定EntryList中的某个线程（一般是最先进入的线程）为OnDeck线程。Owner线程并没有直接把锁传递给OnDeck线程，而是把锁竞争的权利交给OnDeck，让OnDeck重新竞争锁，也叫竞争切换，牺牲了公平性，提高了性能。
        获取到锁资源的OnDeck线程会变成Owner线程，而未获取到锁资源的线程仍然停留在EntryList。
        Owner线程再被wait方法阻塞后，会被转移到WaitSet队列，直到某个时刻被notify或者notifyAll唤醒，会再次进入EntryList。
        ContentionList、EntryList、WaitSet中的线程均为阻塞状态，该阻塞是由操作系统来完成的。
        Owner线程在执行完毕后会释放锁的资源并变成!Owner状态
        为什么synchronized是非公平锁？
            在synchronized中，在线程进入ContentionList之前，等待的线程会先尝试以自旋的方式获取锁，如果获取不到就进入ContentionList，该做法对于已经进入队列的线程是不公平的，因此是非公平锁。
        JDK1.6对synchronized进行了优化，引入了适应自旋、锁消除、锁粗化、轻量级锁以及偏向锁等提高锁的效率。锁从轻量级到重量级的升级过程叫锁膨胀

    ReentrantLock：
        继承了Lock接口并实现了接口中定义的方法，是一个可重入的独占锁。通过自定义AQS（Abstract Queued Synchronized）来实现锁的获取和释放
        独占锁：同一时刻只能被一个线程获取，而获取锁的其他线程只能在同步队列中等待；可重入锁指该锁能够支持一个线程对同一个资源执行多次加锁操作
        支持公平锁和非公平锁。

    ReentrantLock和synchronized的比较：
        共同点：
        1.都用于控制多线程对共享对象的访问
        2.都是可重入锁
        3.都保证了可见性和互斥性
        不同点：
        1.ReentrantLock显式获取和释放锁；synchronized隐式获取和释放锁。使用ReentrantLock时必须在finally语句块中执行释放锁操作
        2.ReentrantLock可响应中断，可轮回，为处理锁提供了更多的灵活性
        3.ReentrantLock是API级别的，synchronized是JVM级别的
        4.ReentrantLock可定义公平锁
        5.ReentrantLock通过Condition可以绑定多个条件
        6.二者的底层实现不一样，synchronized是同步阻塞，采用悲观并发策略；Lock是同步非阻塞，采用乐观并发策略
        7.Lock是一个接口，而synchronized是Java的关键字，synchronized是由内置的语言实现的
        8.通过Lock可以知道是否成功获取到锁（isLock方法），通过synchronized无法做到
        9.Lock可以通过分别定义读写锁提高多个线程读操作的效率

    semaphore：
        基于计数的信号量，在定义信号量对象时可以设定一个阈值，基于该阈值，多个线程竞争获取许可信号，线程竞争到许可信号后开始执行具体的业务逻辑，业务逻辑完成后释放该许可信号。当许可信
        号的竞争队列超过阈值后，新加入的申请许可信号的线程将被阻塞，直到有其他许可信号被释放。
        semaphore常用于多个线程需要共享有限资源的场景
    countDownLatch：
        位于java.util.concurrent包下，是个同步工具类，允许一个或多个线程一起等待其他线程的操作执行完后再执行相关操作
        countDownLatch基于线程计数器来实现并发访问控制，主要用于主线程等待其他子线程都执行完毕后执行相关操作。
        使用过程：
            在主线程定义countDownLatch，并将线程计数器的初始值设置为子线程的个数，多个子线程并发执行，每个子线程在执行完毕后都会调用countDown函数将计数器-1，直到线程计数器=0，表示所有
            的子线程任务都执行完毕，此时在countDownLatch上等待的主线程将被唤醒并继续执行
    cyclicBarrier：
        CyclicBarrier（循环屏障）是一个同步工具，可以让一组线程等待至某个状态后在全部一起执行。并且CyclicBarrier是可以被重用的，运行状态叫Barrier，在调用await后，线程处于Barrier状态
        await有两个实现：
            1.挂起当前线程直到所有的此案从都为Barrier状态在同步执行后继的任务
            2.设置一个超时时间，在超时时间超过后，如果还有线程未达到Barrier状态，则不再等待，让达到Barrier状态的线程继续执行后续任务
        CyclicBarrier为什么可重用？
            CyclicBarrier维护了几个变量，包括parties，count，前者用来表示到达barrier状态的线程个数，count是内部的计数器，两个的初始值第一样的，随着await方法的调用count都会-1，直到为0的时候就将所有线程唤醒，
            可重用的关键点就是会发生一次重置，当全部为0并执行后续操作的时候，会把count再次置为和parties一样的值，这样就达到了可重用的目的。
    三者的区别如下：
        countDownLatch和cyclicBarrier都用于实现多线程之间的相互等待，但二者的关注点不同。countDownLatch主要用于主线程等待其他子线程任务均执行完毕后再执行接下来的业务逻辑单元，
        而CyclicBarrier主要用于一组线程互相等待大家都执行达到某个状态后，在同时执行接下来的业务逻辑单元。此外，countDownLatch是不可重用的，而CyclicBarrier是可重用的
        semaphore和Java的锁功能相似，主要用于控制资源的并发访问。

多线程中共享数据的方法：
    1.将数据抽象成一个类，并将对这个数据额操作封装在类的方法中
    public class MyData() {
        int j = 0;
        // 将数据抽象成myData类，并将对数据的操作作为类的方法.synchronized用作数据的同步
        public synchronized void add(){j++}
        public synchronized void dec(){j--}
        public int getData(){return j;}
    }

    public class AddRunnable implements Runnable {
        MyData myData;
        // 线程使用该类的对象并调用类的方法对数据进行操作
        public AddRunnable(MyData myData) {
            this.myData = myData;
        }
        public void run() {
            myData.run();
        }
    }
    public class DecRunnable implements Runnable {
        MyData myData;
        // 线程使用该类的对象并调用类的方法对数据进行操作
        public AddRunnable(MyData myData) {
            this.myData = myData;
        }
        public void dec() {
            myData.dec();
        }
    }
    public static void main(String [] args) {
        MyData data = new MyData();
        Runnable add = new AddRunnable();
        Runnable dec = new DecRunnable();
        for(int i = 0; i < 2; i++) {
            new Thread(add).start();
            new Thread(dec).start();
        }
    }

    # 上面代码首先定义了一个MyData类，并定义了针对j的数据操作。注意一定要使用synchronized，保证并发下访问对象j进行加锁操作。
      然后定义了DecRunnable和AddRunnable，并把myData通过构造函数传入，线程内部的run函数在执行数据操作的时候直接调用MyData的方法进行操作数据，这样就实现了线程内数据操作的安全性
      注意：如果两个线程DecRunnable和AddRunnable需要保证数据操作的原子性和一致性，就必须保证在传参的时候使用同一个data对象入参，这样无论启动多少个线程执行对data的操作，都能保证数据一致性

    2.将Runnable对象作为一个类的内部类，将共享数据作为这个类的成员变量，每个线程对共享数据的操作都被封装在该类的外部类中，以便实现对数据的各种操作的同步和互斥，作为内部类的各个Runnable对象调用外部类的这些方法
        public class MyData() {
            int j = 0;
            // 将数据抽象成myData类，并将对数据的操作作为类的方法.synchronized用作数据的同步
            public synchronized void add(){j++}
            public synchronized void dec(){j--}
            public int getData(){return j;}
        }

        public class TestThread {
            public static void main(String [] args) {
                final MyData data = new MyData();
                for(int i = 0; i < 2; i++) {
                    new Thread(new Runnable) {
                        public void run() {
                            data.add();
                        }
                    }.start();
                    new Thread(new Runnable) {
                        public void run() {
                            data.dec();
                        }
                    }.start();
                }
            }
        }

    volatile:
        保证变量的同步，还使用了稍弱的同步机制，即volatile。也用于确保将变量的的更新操作通知到其他线程。
        两种特性：
            1.保证该变量对所有线程课件，在一个线程修改了变量的值后，新的值对于其他线程是可以立即获取的。
            2.禁止指令重排。即volatile变量不会被缓存到寄存器中或者对其他处理器不可见的地方，因此在读取volatile类型的变量的时候总能读取到最新写入的值
        访问volatile变量的时候不会执行加锁操作，也就不会执行线程阻塞，因此volatile变量是一种比synchronized更轻量级的同步机制。
        适用于被多个线程共享，多个线程均可针对这个变量执行赋值或者读取的操作
        原理：在有多个线程对普通变量进行读写的时候， 每个线程都首先需要将数据从内存中复制到CPU缓存中，如果计算机有多个CPU，则线程可能都在不同的CPU中被处理，这意味
            着每个线程都需要将同一个数据复制到不同的CPU cache中，这样在每个线程都针对同一个变量的数据做了不同的处理后，就可能存在数据不一致的情况。如果将变量声明为volatile
            ，jvm就能保住每次读取变量的时候都直接从内存读取，跳过CPU cache这步，有效解决了多线程数据同步的问题。
        这里涉及到JMM内存模型和MESI缓存一致性协议、内存屏障，也就是volatile的底层实现
        JMM：
            JMM定义了线程和主内存之间的抽象关系，线程之间的共享变量存储在主内存（Main Memory）中，每个线程都有一个私有的本地内存（Local Memory），本地内存中存储了该线程以读/写共享变量的副本
            ，本地内存是一个抽象概念，并不真实存在。
            Java中jvm对内存模型的具体实现：线程栈区和堆区
            * 每个线程都拥有自己的线程栈，线程栈包含了当前线程执行的方法调用相关信息，我们也把它称作调用栈。随着代码的不断执行，调用栈会不断变化，所有原始类型的局部变量都保存在了线程栈的局部变量表中
            * 各个线程之间的线程栈都是独立的，无法共享，但是可以通过传递变量的副本来进行传送
            * Java堆区包含了Java创建的所有对象信息，比如引用类型，他的引用指针还是在栈区，但是真实的内存开辟是在堆区的，所有对象也存在于堆区

            内存模型带来的问题就是可见性
            CPU中运行的线程从主存中拷贝共享对象obj到它的CPU缓存，比如变量i，对其进行修改，因为缓存的原因，没有立即刷新值到其他的线程本地内存中，导致其他线程读取的时候发生数据不一致的问题
            volatile加上MESI缓存一致性协议，解决了这个问题。
            M(Modified)         这行数据有效，数据被修改了，和内存中的数据不一致，数据只存在于本Cache中
            E(Exclusive)        这行数据有效，数据和内存中的数据一致，数据只存在于本Cache中
            S(Shared)           这行数据有效，数据和内存中的数据一致，数据存在于很多Cache中
            I(Invalid)          这行数据无效

            volatile并不能取代synchronized，他可以保证变量的单次读，写操作的原子性，但并不能保证像i++这种本质上属于读、写的操作

    ConcurrentHashmap：
        和HashMao实现类似，不同的是他采用了分段锁的思想支持并发操作，分段锁的好处是可以减小锁的粒度，ConcurrentHashmap是线程安全的map，最重要的get和set方法都是加了锁的，如果对整个map进行加锁，
        确实可以得到线程安全的对象，但是锁的粒度太大了，意味着同一时刻只能有一个线程操作这个map，所有ConcurrentHashmap内部使用了多个segment，操作数据的时候会给每个segment都加锁，这样就减小了粒度

        实现：
            内部细分为多个HashMap，叫做数据段（segment）。默认分为16个数据段，对每个数据段都是单独加锁，数据段的个数就是锁的并发度。
            ConcurrentHashmap由segment和HashEntry数组组成。segment继承了可重入锁（ReentrantLock），在内部扮演了锁的角色，HashEntry用于存储数据
            segment是数组HashEntry+链表的结构。HashEntry是链表的结构，每个segment守护一个HashEntry数组里面的元素，如果要对数组的元素进行操作，首先要获取数组对应的segment锁

Java中的线程调度：
    1.抢占式调度：每个线程都以抢占的方式获取CPU资源并快速执行，执行完毕后立即释放CPU资源，具体哪些线程能抢占到CPU资源由操作系统决定，这种模式下，每个线程对CPU资源的申请地位都相等
        抢占式调度的实现：为每个线程按照优先级高低分配不同的CPU时间片，优先级高的线程先执行。优先级低的线程只是获取CPU时间片的优先级被降低，不会分配不到时间片
        线程让出CPU的情况如下：
            1.当前运行的线程主动放弃CPU，例如运行中的线程调用yield放弃CPU的使用权
            2.当前运行的线程进入阻塞状态，例如调用文件读取IO操作，锁等待，socket等待
            3.当前线程运行结束，即运行完run方法里面的任务
    2.协同式调度：某个线程在执行完后主动通知操作系统将CPU资源切换到另一个线程上执行，线程对CPU的持有时间由线程自身控制，线程切换更加透明，更适合多个线程上交替执行某些任务的情况
        协同式调度的缺点：如果其中一个线程因为外部原因（磁盘IO阻塞、网络IO阻塞等）运行阻塞，那么可能导致整个系统阻塞甚至崩溃

    进程调度算法：
        1.优先调度算法
            1.1 先来先服务调度算法
            每次调度时，都从队列里选择一个或多个最早进入该队列的作业，为其分配资源，创建进程和放入就绪队列。调度算法在获取到可用的CPU资源时会从就绪队列中选择一个最早进入队列的进程，
            为其分配CPU资源并允许。该算法优先运行最早进入的任务，实现简单且相对公平
            1.2 短作业优先调度算法
            每次调度时都从队列中选择一个或若干个预估运行时间最短的作业，为其分配资源，创建进程和放入就绪队列。调度算法在获取到可用的CPU资源时，会从就绪队列中选出一个预估运行时间最短的进程，
            为其分配CPU资源并运行。该算法优先运行短时间作业，以提高CPU整体的利用率和系统允许效率，某些大任务可能会出现长时间得不到调度的情况

        2.高优先权优先调度算法
            在定义任务的时候为每个任务设置不同的优先权，在进行任务调度时优先权最高的任务首先被调度，这样资源的分配将更加灵活，具体包含非抢占式优先调度算法、抢占式优先调度算法和高响应比优先调度算法
            1.1 非抢占式优先调度算法
            每次调度时都从队列中选择一个或多个优先权最高的作业，为其分配资源，创建进程和放入就绪队列。
            1.2 抢占式优先调度算法
            首先把CPU资源分配给优先权最高的任务并运行，但如果在运行过程中出现比当前运行任务优先权更高的任务，调度算法就会暂停运行该任务并回收CPU资源，为其分配新的优先权更高的任务
            1.3 高响应比优先调度算法
            高响应比优先调度算法使用了动态优先权的概念，即任务的执行时间越短，其优先权越高，任务的等待时间越长，优先权越高，这样即保障了快速、并发的执行短作业，也保障了优先权低但长时间等待的任务也有被调度的可能
                优先权变化规律：
                    1. 在作业的等待时间相同时，运行时间越短，优先权越高，这种遵循短作业原则
                    2. 在作业的运行时间相同时，等待时间越长，优先权越高，这种遵循先来先服务原则
                    3. 随作业等待时间的增加而不断提高，加大了长作业获取CPU资源的可能性
        3.时间片的轮转调度算法
            1.时间片轮转法
            按照先来先服务原则从就绪队列中取出一个任务，并为该任务分配一定的CPU时间片去运行，在进程使用完CPU时间片后由一个时间计时器发出时钟中断请求，调度器在收到时钟中断请求信号后停止该进程放入
            就绪队列的队尾，然后从就绪队列的对首取出一个任务并为其分配CPU时间片去执行
            2.多级反馈队列调度算法
            在时间片轮询算法的基础上设置多个就绪队列，，并为每个就绪队列设置不同的优先权。队列的优先权越高，队列中的任务被分配的时间片就越大。默认第一个队列优先权最高，其他次之
            调度流程：
                在系统收到新的任务时，首先将其放入第一个就绪的队列，按先来先服务调度算法排队等待调度，若该进程在规定的CPU时间片内完成了执行或出现了错误，则退出进程并从系统中移除。如果规定时间片内没有完成，
                则该进程转入第二队列的队尾等待调度执行，如果第二队列中还是没有执行完成，则转到第三队列的队尾等待执行，以此类推，直到完成。

CAS：
    概念：比较并交换（Compare And Swap）CAS(V, E, N) V表示需要更新的变量，E表示预期值，N表示新值
    仅在V值等于E值时，才会将V值设置为N值。如果V值和E值不同，则说明已经有其他线程做了更新，当前线程什么都不做，最后，CAS返回当前V的值
    特性： 乐观锁，总是认为自己可以成功完成操作。
    CAS自旋等待：
        例如java.util.concurrent.atomic中的一组原子类。其内部就是基于CAS来实现的，在某个线程进入方法中执行其中的命令时，不会被其他线程打断，而别的线程就像自旋锁一样，一直等到
        该方法执行完成才由JVM从等待的队列中选择另一个线程进入。
        public class AtomicInteger extends Number implements java.io.Serializable {
            private volatile int value;
            public final int get() {
                return value;
            }
            public final int getAndIncrement() {
                for(;;) { // CAS自旋，一直尝试，直到成功
                    int current = get();
                    int next = current + 1;
                    if (compareAndSet(current, next)) {
                        return current;
                    }
                }
                public final boolean compareAndSet(int expect, int update) {
                    return unsafe.compareAndSwapInt(this, valueOffSet, expect, update);
                }
            }
        }

    ABA问题：
        CAS算法的重要前提：需要取出内存中某时刻的数据，然后在下一时刻进行比较、替换，在这个时间差可能数据已经发生了变化，导致ABA问题
        第一个线程从内存的V位置取出A，这时候第二个线程也从内存中取出A，并将V位置的数据首先修改为B，接着又将V位置的数据修改为A，这时第一个线程在进行CAS操作时会发现在内存里
        仍然是A，然后第一个线程操作成功。尽管这个操作成功了，但在这个过程中其实V的数据已经发生了变化了，只是第一个线程没有感知到，在某些情况下会出问题（例如银行账户的问题）
        # 一部分的乐观锁通过版本号（version）来解决ABA问题，具体的操作是乐观锁每次在执行数据的修改操作的时候都会带上一个版本号，在预期的版本号和数据的版本号一致的时候就可以执行修改操作，
        并对版本号+1，否则执行失败。因为版本号每次都会增加，所以不会出现ABA问题
        # AtomicStampedReference解决ABA问题
            有一个int 值作为版本号，每次更改前先取到这个int值的版本号，等到修改的时候，比较当前版本号与当前线程持有的版本号是否一致，如果一致，则进行修改
            ，并将版本号+1（当然加多少或减多少都是可以自己定义的），在zookeeper中保持数据的一致性也是用的这种方式
            AtomicStampedReference的compareAndSet函数的四个参数
            （1）第一个参数expectedReference：表示预期值。
            （2）第二个参数newReference：表示要更新的值。
            （3）第三个参数expectedStamp：表示预期的时间戳。
            （4）第四个参数newStamp：表示要更新的时间戳。
        # AtomicMarkableReference解决ABA问题 维护一个boolean类型的标记
            将一个boolean值作是否有更改的标记，本质就是它的版本号只有两个，true和false，修改的时候在这两个版本号之间来回切换
            ，这样做并不能解决ABA的问题，只是会降低ABA问题发生的几率而已

AQS:
    （Abstract Queued Synchronizer）抽象的队列同步器，通过维护一个共享访问状态资源（Volatile Int State）和一个先进先出（FIFO）的线程等待队列来实现一个多线程访问共享资源的同步框架
    原理：AQS为每个共享资源都设置一个共享资源锁，线程在需要访问共享资源时首先需要获取共享资源锁，如果获取到了共享资源锁，便可以在当前线程中使用该共享资源，如果获取不到，则将该线程放入线程
    等待队列，等待下一次资源调度。

    state：状态
    AQS维护了一个volatile int类型的变量，用于表示当前的同步状态。volatile可以保证当前变量state的可见性。
    state的三种访问方式：
    // 返回共享资源状态，此操作的内存语义为volatile修饰的原子读操作
    protected final int getState() {
        return state;
    }
    // 设置共享资源状态，此操作的内存语义为volatile修饰的原子写操作
    protected final void setState(int newState) {
        state = newState;
    }
    // 自动将同步状态设置为给定的更新状态值（如果当前的状态值等于预期值）
    // 此操作的内存语义为volatile修饰的原子读写操作
    protected final boolean compareAndSetState(int expect, int update) {
        return unsafe.compareAndSwapInt(this, stateOffSet, expect, update);
    }

    AQS共享资源的方式：独占式和共享式
    独占式：只有一个线程能执行，具体的Java实现由ReentrantLock
    共享式：多个线程可同时执行，具体的Java实现有Semaphore和CountDownLatch

    AQS的核心是同步器，具体线程等待队列的维护AQS已经实现好了，开发人员只需要实现自定义的同步器：
        isHeldExclusively()                          查询该线程是否正在独占资源，只有用到condition才需要实现该方法
        tryAcquire(int)         独占                  尝试获取资源：成功返回true，失败则false
        tryRelease(int)         独占                  尝试释放资源：成功返回true，失败则false
        tryAcquireShared(int)   共享                  尝试获取资源：负数代表失败；0代表成功，但没有可用资源；整数表示成功，且有剩余资源
        tryReleaseShared(int)   共享                  尝试释放资源：如果释放资源后允许唤醒后续等待线程，则返回true，否则返回false

    ReentrantLock对AQS的独占方式的实现：
        ReentrantLock中state初始值为0时表示无锁状态.在线程执行tryAcquire获取该锁后ReentrantLock中的state+1，这时该线程独占ReentrantLock锁，其他线程在通过tryAcquire获取
        锁时，均会失败，直到该线程释放锁后state再次为0，其他线程才有机会获取该锁。该线程在释放锁之前可以重复获取该锁，每获取一次便会执行一次state+1，因此ReentrantLock也属于可重入锁
        但获取多少次锁就要释放多少次锁，这样才能保证state最终为0。如果获取锁的次数多于释放锁的次数，则会出现该线程一直持有该锁的情况：如果获取锁的次数少于释放锁的次数，则运行中程序会抛出锁异常！
    CountDownLatch对AQS的共享方式的实现：
        CountDownLatch将任务分为N个子线程去执行，将state也初始化为N，N与线程的个数一致，N个子线程是并行执行的,每个子线程都会执行完后countDown一次，state会执行CAS操作并-1.
        在所有的子线程都执行完成后，此时state=0，会开始unpack()主线程，然后主线程会从await()返回，继续执行后续的动作

    一般来说，自定义同步器要么独占，要么共享，也支持同时实现独占和共享，比如最著名的ReentrantReadWriteLock读取的时候才用共享，写入时采用独占







